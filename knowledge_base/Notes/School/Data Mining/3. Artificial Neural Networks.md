# Perceptron (aka neuron)
Supervised learning
Mathematical function with n inputs with weights associated with each. It has another input called the bias.
1. Calculate weighted sum of inputs and add bias
2. Apply non-linear function (sigma/activation function) and output the result
Supervised learning algorithm of binary classifiers (output is 0 or 1)
Linear classifier
### Activation function
Range: \[0,1]
As you can see, there are in-between values as the function transitions between states
Examples:
![[activation_functions.jpg]]
## Decision Boundary
If you plot on a graph the sum of the weighted inputs and the bias, you get a line. This line is the decision boundary
## Multilayer Perceptron
Can combine many perceptrons together in layers. In general, this is an example of a multilayer artificial neural network:
![[0__SH7tsNDTkGXWtZb.png]]
Using multilayer perceptron for classification. Each input is a feature/attribute value. For a binary class dataset, can feed all outputs into a single neuron and have it output 0 or 1.
## Feed-Forward Neural Network
One-way flow from input to output
## Recurrent Neural Network
Loops are allowed, causing effectively an infinite number of layers
## Learning
Determine the weights and bias to get the correct output for the specified inputs
With the training data, we know what output to expect for given inputs
Initialize with random weights
For each training record:
1. Compute the output given the features of the record
2. Update each weight based on the error of the output
$$
w_j^{k+1}=w_j^k + \lambda(y_i - \hat{y}_i^k)x_{ij}
$$
- j is the dimension ("the jth weight")
- lambda is the learning rate
	- If learning rate is too high, the weights will never converge
	- If learning rate is too low, the weights will be very slow to converge
	- Should be a value between 0 and 1
	- Can use an adaptive value (changes over time as needed)
- y_i - y_i^k is the error
- xij is the jth dimension of the input record xi
Stopping condition:
- Stop after the first pass over all training samples
- Do p passes over all training samples (epoch)
	- Do a certain amount
	- Stop when the weights stop changing above a certain threshold

# Backpropagation - Learning for Artificial Neural Networks
Can't use the perceptron model for learning because we can't see the error for the hidden layers
Hard to find global minima/maxima. Models can easily become locally optimized
## Gradient descent
A method for finding local maxima-minima. Example uses a function with respect to theta
$$
\theta := \theta - \alpha \frac{d}{d\theta}J(\theta)
$$
Error of a particular weight
$$
E(w)=\frac12 \sum_{i=1}{N}(y^i-\hat{y})^2
$$
- This is the function we want to minimize
## Process
1. Perform a forward pass
2. Find the error for that training sample
	$$
	E_i = \frac12(y_i-\hat{y}_i)^2
	$$
3. Look at the error as the weighted sum of the errors in input to that last node
![[artificial-neural-network3.png]]
$$
E = w_1E_1 + w_2E_2 + ... + w_nE_n
$$
$$
\frac{\partial E}{\partial w_1} = E_1
$$

$$
\frac{\partial E}{\partial w_n} = E_n
$$

# Process
1. Init weights with random values
2. Perform a forward pass
3. Compute error (predicted - actual)
4. Calculate all weights from hidden layer to output layer
5. Calculate all weights from input layer to hidden layer
6. Update weights
Do this until all examples are classified correctly or until a stopping condition is met
Backpropagation is steps 4-6

# Characteristics
- More layers means more epochs to train
- Redundant features can be used as weights
- Gradient descent to optimize. Challenge is to find a way out of local maxima-minima
- Sensitive to noise
- Complex
- Time consuming

Multi-layer perceptron (ANN) is a non-linear classifier
Perceptron/ANN is supervised learning
Artificial Neural Network is essentially a complex function
Cannot use perceptron learning algorithm due to not knowing the true error at each hidden layer. Therefore we need to use backpropagation