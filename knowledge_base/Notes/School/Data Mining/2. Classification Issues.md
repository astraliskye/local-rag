# Errors
Training/re-substitution error: number of errors on training dataset
Generalization error: expected error on the test dataset
Overfitting: fits training data very well due to added complexity but has poor generalization error
- Training error close to 0
- Generalization error very high
Underfitting: model is too simple
Occam's Razor: prefer the simpler model given similar generalization errors
- Less compute
- Less likely to be fit to noise/errors
Generalization errors
- Optimistic: training data has the same error as the training data
- Pessimistic: incorporates model complexity
	- i.e. the amount of leaf nodes in a decision tree. N = number of leaf nodes. Leaf node penalty = 0.5
	$$
e'(T) = e(T) + N \cdot 0.5
$$
Addressing overfitting/underfitting
- Pre-pruning
	- i.e. stopping the decision tree at an appropriate time
- Add more conditions for stopping training
- Post-pruning
	- Trim nodes from a decision tree in a bottom-up fashion
# Bias vs Variance
Bias: how much the average model over all training sets differ from the true model
- Due to assumptions or simplifications made by model
Variance: how much models differ which are trained on different training data sets
