Data mining is extracting non trivial, previously unknown, and implicit knowledge from data. It can also be defined as exploration or analysis of data using some sort of automation.

Data mining tasks can either be predictive - using certain variables to predict the future value of other variables - or descriptive - finding patterns in current data

Predictive tasks
- Classification
- Regression
- Deviation detection
Descriptive tasks
- Clustering
- Association rule discovery

Feature: a variable that describes a system
Data sample: the set of all features for a system
- Dimension: how many features a data sample has
Feature matrix: set of data samples organized in rows
Class label vector: vector with as many elements as the feature matrix has rows. Each element is a label for the corresponding data sample in the feature matrix

Data matrix: feature matrix plus the label vector as the last column.
- Dimension is N x D+ 1
- D is the number of features in the feature matrix. We add 1 due to the concatenation of the label vector

Attribute (feature type): a characteristic of an object
Attribute value (feature): the value of that characteristic for a particular object
Object: a collection of attributes
Data: a set of data objects and their attributes

Types of attributes
- Nominal: a name
	- Distinctness
- Ordinal: something with an order
	- Distinctness and order
- Interval: a range
	- Distinctness, order, and addition
- Ratio
	- A length is a ratio: ex. 3 centimeters -> 3/1 centimeters
	- Distinctness, order, addition, and multiplication

Attribute properties
- Distinctness: equal or not equal
- Order: less than, greater than or equal
- Addition
- Multiplication

Transformation: an operation that takes an attribute value as input and outputs an attribute value of the same type
- Nominal: any type of permutation
- Ordinal: any order-preserving change of values
- Interval: y = mx + b
- Ratio: y = mx

Attributes can be discrete or continuous
- Discrete has finite or countable infinite values
- Continuous has uncountably infinite values, but can only realistically be represented with a finite number of digits

Asymmetry of attributes
- Symmetric attribute: all values are equally important
- Asymmetric attribute: some values are more or less important. E.g. if we only care if an attributeâ€™s value is zero vs non-zero

Types of **structured** data sets
- Record - can be represented as a matrix
	- Transaction data
- Graph - nodes and edges
	- Chemical data (molecule structure, which atoms are bonded)
- Ordered - data points have some sort of order, e.g. spatial or temporal
- Document - how many occurrences of each word appear in each document

Characteristics of structured data
- Dimension - number of columns
	- Higher dimension means more information, but also more processing power and increased search space (can cause algos to get confused)
- Resolution - higher sampling rate (for time series data)
- Sparsity - how dense relevant values are. E.g if we care if a value is non-zero and most of the values are zero, then the set is sparse

Data quality
- Noise
	- Modification of original values
- Duplicate values
	- Do you take one over the other or use one to improve the other
- Outliers
	- Can be noise or can be erroneous assumptions about the system
- Missing values
	- Can substitute zeroes, ignore records, etc

Preprocessing
- Aggregation - combining multiple values or objects
- Sampling - choosing a smaller subset of data objects
	- Choose representative samples
	- It is inefficient to consider too large a sample size
- Lowering dimensions
	- Easier to visualize if 3 or less dimensions
	- Easier to compute and store
	- Potentially lower search space
	- Better clustering (meaning better outlier detection)

Lowering dimensions
- PCA (Principal Component Analysis)
	- Goal is to find an axis projection that captures the largest variance in data
	- Attributes with high standard deviation are considered very important 
	- Find the eigenvectors of the covariance matrix of the data

Measure distance
- Euclidean distance: sum the squares of the differences of components, then take the square root
$$
\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$
-  Minkowski distance: extension of Euclidean distance. Different values for root and power
$$
\sqrt[r]{(x_2 - x_1)^r + (y_2 - y_1)^r}
$$
	- L1 norm: r = 1
	- L2 norm: Euclidean distance
- Properties of a distance function
	- Greater than zero between all points
	- Equal to zero only if they are the same points
	- Commutative
	- Triangle inequality proptery
		- D(a, c) <= D(a, b) + D(b, c)

IDs are not useful attributes

# Similarity
- How similar two data objects are
- Similarity(a, b) = 1 if a is b
- Commutative

## Binary vector similarity
### Simple Matching Coefficient
	- Number of matches / number of attributes
	- Can indicate false similarity if there are a bunch of missing components in both vectors and zeroes have been substituted for those by default
	- Can be good for comparing if you want to include sparsity
### Jaccard Coefficient
	- Number of 1-1 matches / number of non-0-0 matches

## Cosine similarity
$$
cos(u, v) = \frac{u \cdot v}{||u||*||v||}
$$
- Does not account for magnitude

## Extended Jaccard Coefficient (for real numbers, not just binary)
$$
\frac{p \cdot q}{||p||^2 + ||q||^2 - p \cdot q}
$$
- p dot q / (||p||^2 + ||q||^2 - p dot q)

## Correlation(super important)

Variance of a vector v
$$
S_v = \sqrt{\frac{1}{n - 1} \sum_{i=1}^n (v_i - \overline{v})^2}
$$
Covariance of vectors v and u
$$
s_{vu} = \frac{1}{n - 1} \sum_{i=1}^n (v_i - \overline{v})(u_i - \overline{u})
$$
Correlation of vectors v and u
$$
corr(v, u)=\frac{s_{vu}}{s_v * s_u}
$$
	- Correlation only cares about linear relationships
	- Range is -1 to 1
	- Zero is no correlation
	- -1 and 1 are high correlation

## Combining similarities
1. For the kth attribute, compute a similarity s_k in the range [0,1]
	1. Can use Jaccard coefficient, covariance, extended Jaccard coefficient, etc.
2. Define an indicator variable delta_k
$$
\delta_k = 
\begin{cases}
	0 & \text{if the } k^{th} \text{ attribute is a binary asymmetric attribute and both objects have a value of 0, or if one of the objects has missing values for the } k^{th} \text{attribute} \\
	1 & othewise
\end{cases}
$$
3. Compute the overall similarity between the two objects using the following formula
$$
similarity(u, v) = \frac{\sum_{k=1}^n \delta_k s_k}{\sum_{k=1}^n \delta_k}
$$

Using weights to combine similarities
- Weights have to sum up to 1
$$
weighted\_similarity(u, v) = \frac{\sum_{k=1}^n w_k \delta_k s_k}{\sum_{k=1}^n \delta_k}
$$
$$
weighted\_distance(u, v) = \sqrt[r]{\sum_{k=1}^n w_k |u_k-q_k|^r}
$$
# Density
## Cell-Based (Euclidean)
Divide the space into boxes (cells)
Calculate number of data objects in those boxes
Build a matrix with the number of dots inside the corresponding box
Higher values mean higher density

## Center-Based
Define a radius around each data object
Compute the amount of data objects within that radius
The number of data objects in the radius is the density of the data point