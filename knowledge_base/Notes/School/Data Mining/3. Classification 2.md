# Instance-Based Classifiers
Feature space: spatial representation of data objects where each axis corresponds to a feature
## Rote-learner
Memorizes training data. Only classifies new records if it matches exactly some previously seen record
## Nearest Neighbor
Uses k "closest" points for performing classification. If there are an equal number of different classes in that set of points, the tie is broken by which set of points is closest

Choosing k:
- Too small: sensitive to noise points
- Too large: may include points from other classes

Choosing distance function:
- Euclidean distance
- Manhattan distance

Scaling issues: some attributes are on different scales, meaning that some attributes affect the result more than others

Curse of dimensionality: as the number of attributes increases, the distance between classes decreases

Curse of dimensionality googled definition: number of features significantly exceeds the number of observations
- Data sparsity
- Increased computational complexity
- Difficulties finding meaningful patterns

k-NN classifiers cons
- Does not build models explicitly
- Classifying unknown records is relatively expensive
k-NN classifier pros
- Does not need to be trained
## PEBLS: Parallel Examplar-Based Learning System
Distance between two nominal attribute values in a data set with two possible classes
V1 and V2 are attribute values, i iterates over each class
$$
d(V_1, V_2) = \sum_i|\frac{n_{1i}}{n1} - \frac{n_{2i}}{n2}|
$$
Distance between two data points
X and Y are data points. X_i and Y_i are attribute values
$$
\Delta(X,Y)=w_Xw_Y\sum_{i=1}^dd(X_i,Y,i)^2
$$
$$
w_X=\frac{\text{Number of times X is used for prediction}}{\text{Number of times X predicts correctly}}
$$
## Naive Bayes classifier
Multinomial & Gaussian versions

Experiment: set of steps that produces one out of several possible outcomes
- i.e. tossing a coin and having it land on heads of tails
Sample space: set of all possible outcomes
Event: subset of the sample space
Random variable: sampling from the sample space
- Evolves as more samples are taken
### Multinomial Naive Bayes Classifier
1. Assign probabilities to each event, separated by class
	1. Build a histogram of each event
	2. If there are zero occurrences of one or more events, add one occurrence to each event, such that all events have a non-zero number of occurrences (called Laplace calculation)
	3. For each event, divide the number of occurrences by the number of occurrences of every event
2. Classify new event
	1. For each class, assume the event is of that class and assign it a score, typically by using previously calculated probabilities
Naive Bayes is naive because it doesn't take into account the order of events when taking multiple events into consideration

Gaussian Naive Bayes: rather than a histogram for the distribution, use a curve

Probability of a single event occurring is the number of times that event appears in the sample space divided by the size of the sample space

Prior/marginal probability: prior knowledge of an event occurring, not conditional on any other event
Joint probability: probability that two events occur simultaneously
$$
P(A,B)=P(A|B)\cdot P(B)=P(B|A) \cdot P(A)
$$
Conditional probability: probability that an event will occur given another event has occurred
$$
P(A|B)=\frac{P(A,B)}{P(B)}
$$
Bayes Theorem: inverting conditional probability. Find the probability of a cause given its effect
	$$
P(Y|X)=\frac{P(X|Y)\cdot P(Y)}{P(X)}
$$
- ex: a doctor knows meningitis causes stiff neck 50% of the time. 1 in 50,000 patients have meningitis. 1 in 20 patients have stiff neck
$$
	P(S | M) = 0.5
$$
$$
P(M|S) = \frac{P(S|M)\cdot P(M)}{P(S)} = \frac{0.5 \cdot 0.00002}{0.05} = 0.0002
$$
Law of total probability: fundamental rule relating marginal probabilities to conditional probabilities. Suppose A has n "causes." Quotes because B_n doesn't have to cause A, we're just given B_n
	$$
P(A) = \sum_n P(A|B_n) \cdot P(B_n)
$$
- ex: Two factories X and Y make lightbulbs. Factory X's lightbulbs work for over 5000 hours in 99% of cases and 60% of total lightbulbs are manufactures by factory X. Factory Y's lightbulbs work for over 5000 hours in 95% of cases. Given you bought a lightbulb, what is the chance it will last for over 5000 hours
$$
P(\text{over 5000 hours}) = P(\text{over 5000 hours} | X) \cdot P(X) + P(\text{over 5000 hours} | Y) \cdot P(Y)
$$
$$
P(\text{over 5000 hours}) = 0.99 \cdot 0.6 + 0.95 \cdot 0.4 = 0.974
$$
Conditional independence: 
- Given events X, Y, and Z. X is conditionally independent of Y given Z if this equation holds true. i.e. X is actually dependent on Z
	$$
P(X|Y,Z) = P(X|Z)
$$
Classification
- Given a record with attributes X_1, X_2, ..., X_n
- Goal is to predict class C
- For each class, calculate the probability that the record is that class. Whichever probability is highest, the record is that class
- Naive: assumes attributes are conditionally independent
	$$
P(X_1, X_2, ..., X_n|C) = P(X_1|C)\cdot P(X_2 | C) \cdot ... \cdot P(X_n | C)
$$
$$
P(X|C) = \prod_{i=1}^n P(X_i|C)
$$
Calculating probabilities of continuous variables:
- Discretize domain of the variable
- Assume a gaussian distribution
#### Probability Mass Function
For discrete random variables
It is the probability that a random variable X can assume a value x
S is the sample space
x is a single event in the sample space
f(x) is the probability mass function for a single event
A is some subset of sample space S
Properties: P(X = x) = f(x) >= 0
- Sum of f(x) for all x in S is 1
- P(X belongs to A) is the sum of f(x) for all x in A
#### Probability Density Function
Probability that a continuous random variable can assume a value within a given interval
PDF of a continuous random variable X is an integrable function which satisfies these properties:
- P(X = x) = f(x) >= 0 for all x in the sample space
	- Integral over 2dx (very small interval)
- Integral over the sample space for f(x) is 1
- P(X belongs to A) = the integral over A of f(x)
#### Probability Distribution
Integral of the probability density function
Normal distribution
Integral over much larger interval that 2dx
### Naive Bayes Classifier Finishing Notes
- Correlated attributes can degrade performance because the conditional independence assumption no longer holds
- Robust to irrelevant attributes
- Robust to isolated noise points
## Support Vector Machine
